---
layout: default
title:  "MATH1231 Notes"
date:   2018-11-03 10:15:00 +1100
permalink: uni/1231-notes
category: uni
hidden: true
tags:
  - 1231
  - tex
  - linear algebra
  - calculus
  - mathematics 1B
color: 3D6117
comments: true
---

<small style="color: #777; top: -10px; position: relative">
  Written by <a href="https://github.com/lukakerr">Luka Kerr</a> on November 3, 2018
</small>

## MATH1231 Notes

For my math courses at uni, I used LaTeX to write notes. Below is the algebra and calculus `.tex` sources for my MATH1231 notes.

You can find `.pdf` versions for the [algebra notes here]({{ "/assets/static/1231-algebra-notes.pdf" | absolute_url }}) and [calculus notes here]({{ "/assets/static/1231-calculus-notes.pdf" | absolute_url }}).

### Algebra

```tex
\documentclass[a4paper,12pt]{article}
\usepackage{fullpage, setspace, amsmath, amsfonts, amssymb, hyperref}
\providecommand{\tightlist}{\setlength{\itemsep}{8pt}\setlength{\parskip}{-6pt}
}
\setlength\parindent{0mm}
\begin{document}
\onehalfspacing{}

\section*{MATH1231 Algebra Summary}

\subsection*{Subspaces}

A subspace $S$ is only a subspace of $V$ over a field $\mathbb{F}$ if:

\begin{itemize}
  \tightlist{}
  \item The zero vector $\vec{0}$ belongs to $S$
  \item $S$ is closed under vector addition
  \item $S$ is closed under scalar multiplication
\end{itemize}


\subsection*{Linear Combinations and Spans}

Given a set of vectors $S = \{ v_1, v_1 \dots{} v_n \}$:

\begin{itemize}
  \tightlist{}
  \item A \textbf{linear combination} of $S$ is a sum of scalar multiples of
  the form $\lambda_1 v_1 + \dots{} \lambda_n v_n$
  \item A \textbf{span} of $S$ is the set of all linear combinations of $S$
  \item A \textbf{spanning set} is the finite set $S$ such that every vector in
  a vector space $V$ can be expressed as a linear combination of vectors in $S$
\end{itemize}


\subsection*{Linear Independence}

\subsubsection*{Linearly Independent}

A set of vectors $S$ is said to be linearly independent if for each vector in
$S$, the only values of the scalars $\lambda_1, \lambda_2 \dots{} \lambda_n$
for which $\lambda_1 v_1 + \lambda_2 v_2 + \dots{} + \lambda_n v_n = 0$, are
$0$.

\subsubsection*{Linearly Dependent}

A set of vectors $S$ is said to be linearly dependent if it is not a linearly
independent set. In other words, there exists a $\lambda_n$ that is not $0$.


\subsection*{Basis and Dimension}

A set of vectors $S$ is a basis for a vector space $V$ if:

\begin{itemize}
  \tightlist{}
  \item $S$ is a linearly independent set
  \item $S$ is a spanning set for $V$
\end{itemize}


\subsection*{Linear Maps}

A function $T : V \to W$ is called a \textbf{linear transformation} if:

\begin{itemize}
  \tightlist{}
  \item Addition is preserved: $T(\bold{v} + \bold{u}) = T(\bold{v}) +
T(\bold{u})$
  \item Scalar multiplication is preserved: $T(\lambda \bold{v}) = \lambda
T(\bold{v})$
\end{itemize}


\subsection*{Linear Map Subspaces}

\subsubsection*{Kernel}

\textit{Definition:} $ker(A) = \{ \bold{v} \in \mathbb{R}^n : A \bold{v} = 0
\}$

\textit{Calculate:} row reduce $A$, solve for $\lambda$'s and form a span

\textit{Basis:} form a set using the smallest number of linearly
independent vectors from $ker(A)$

\subsubsection*{Nullity}

\textit{Definition:} $nullity(A) = dim(ker(A))$

\textit{Calculate:} find $ker(A)$ and take its dimension OR take $dim(A)
- rank(A)$

\subsubsection*{Image}

\textit{Definition:} $im(A) = \{ \bold{b} \in \mathbb{R}^n : A \bold{x} =
\bold{b} \}$

\textit{Calculate:} row reduce $(A|b)$

\textit{Basis:} row reduce, find the linearly independent columns and take
those columns from the original matrix as a span

\subsubsection*{Rank}

\textit{Definition:} $rank(A) = dim(im(A))$

\textit{Calculate:} find $im(A)$ and take its dimension OR row reduce $A$ and
take the number of leading columns


\subsection*{Eigenvalues and Eiegnvectors}

\subsubsection*{Eigenvalue}

\textit{Definition:} $\lambda$'s such that $A \bold{v} = \lambda \bold{v}$

\textit{Calculate:} solve $det(A - \lambda I) = 0$ where $I$ is the identity
matrix for $A$

\subsubsection*{Eigenvector}

\textit{Definition:} $\bold{v}$ such that $A \bold{v} = \lambda \bold{v}$ for
each eigenvalue $\lambda$

\textit{Calculate:} solve $ker(A - \lambda I)$ for each eigenvalue $\lambda$


\subsection*{Diagonalisation}

A square matrix $A$ is said to be diagonalisable if there exists an invertible
matrix $M$ and a diagonal matrix $D$ such that $M^{-1} A M = D$. \\

Given $n$ linearly independent eigenvectors $\bold{v_1}, \dots, \bold{v_n}$ and
corresponding eigenvalues $\lambda_1, \dots, \lambda_n$, we let

$$
M = (\bold{v_1} | \dots | \bold{v_n})
, \quad
D = \begin{pmatrix}
  \lambda_1 & & 0 \\
   & \ddots & \\
   0 & & \lambda_n
\end{pmatrix}
$$

such that $M^{-1} A M = D$ holds.


\subsection*{Systems of Differential Equations}

The system
$
\begin{cases}
  \frac{dx}{dt} = a_1 y_1 + b_1 y_2 \\
  \frac{dy}{dt} = a_2 y_2 + b_2 y_2
\end{cases}
$
can be written as
$
\begin{pmatrix}
  a_1 & b_1 \\
  a_2 & b_2
\end{pmatrix}
\begin{pmatrix}
  y_1 \\
  y_2
\end{pmatrix}
$

where
$
\begin{pmatrix}
  y_1 \\
  y_2
\end{pmatrix}
=
c_1 \bold{v_1} e^{\lambda_1 t} + c_2 \bold{v_2} e^{\lambda_2 t}
$
given that $\lambda_k, \bold{v_k}$ are eigenvalue-eigenvector pairs.


\subsection*{Probability}

\subsubsection*{Rules and Conditions}

\begin{enumerate}
  \item $P(A \cap B) = P(A) + P(B) - P(A \cup B)$
  \item $P(A^c) = 1 - P(A)$
  \item $P(A|B) = \dfrac{P(A \cap B)}{P(B)}$
  \item Mutual exclusion if $A \cap B = \emptyset$
  \item Statistically independent if $P(A \cap B) = P(A) \times P(B)$
\end{enumerate}


\subsection*{Random Variables}

\subsubsection*{Probability Distribution}

To show a sequence $p_k$ is a probability distribution the following properties
must be proven

\begin{enumerate}
  \item $p_k \ge 0$
  \item $\sum_k^{\infty} p_k = 1$
\end{enumerate}

\subsubsection*{Expected Value}

$E(X) = \sum_{\text{all k}} k p_k$

$E(X^2) = \sum_{\text{all k}} k^2 p_k$

\subsubsection*{Variance}

$Var(X) = E(X^2) - E(X)^2$

\subsubsection*{Standard Deviation}

$SD(X) = \sqrt{Var(X)}$


\subsection*{Special Distributions}

\subsubsection*{Binomial Distribution}

$B(n, p, k) = \begin{pmatrix}n \\ k\end{pmatrix} p^k (1 - p)^{n - k}$ where
$k = 0, 1, \dots{} n$

\subsubsection*{Geometric Distribution}

$G(p, k) = (1 - p)^{k - 1} p$


\subsection*{Continuous Random Variables}

A random variable $X$ is continuous if $F_X(x)$ is continuous.

\subsubsection*{Probability Density Function}

The probability density function of a continuous random variable $X$ is
defined by

$$
f(x) = f_X(x) = \dfrac{d}{dx} F(x) \ , \ x \in \mathbb{R}
$$

if $F(x)$ is differentiable, and $\lim_{x \to a^-} \dfrac{d}{dx} F(x)$ if
$F(x)$ is not
differentiable at $x = a$.

\subsubsection*{Expected Value}

$E(X) = \displaystyle \int_{- \infty}^{\infty} x \ f(x) \ dx$

\subsubsection*{Variance}

$Var(X) = E(X^2) - (E(X))^2$


\subsection*{Special Continuous Distributions}

\subsubsection*{Normal Distribution}

A continuous random variable $X$ has a normal distribution $N(\mu, \sigma^2)$
if it has a probability density $\phi (x) = \dfrac{1}{\sqrt{2 \pi \sigma^2}}
e^{- \frac{1}{2} (\frac{x - \mu}{\sigma})^2}$ where $- \infty < x <
\infty$.

\subsubsection*{Exponential Distribution}

A continuous random variable $T$ has an exponential distribution $Exp(\lambda)$
if it has a probability distribution density

$$
f(t) =
\begin{cases}
  \lambda e ^{- \lambda t} & \text{if  t} \ge 0 \\
  0 & \text{if  t} < 0
\end{cases}
$$

\end{document}
```

### Calculus

```tex
\documentclass[a4paper,12pt]{article}
\usepackage{fullpage, setspace, amsmath, amsfonts, amssymb, hyperref}
\providecommand{\tightlist}{\setlength{\itemsep}{8pt}\setlength{\parskip}{-6pt}
}
\setlength\parindent{0mm}
\begin{document}
\onehalfspacing{}

\section*{MATH1231 Calculus Summary}

\subsection*{Partial Differentiation}

To find the partial derivative of a function with two variables $x$ and $y$, we
can treat one of the variables as a constant and differentiate with respect to
the other.


\subsection*{Tangent Plane To Surfaces}

Suppose $F$ is a function of two variables, and $P$ is a point $(x_0, y_0,
z_0)$ that lies on the surface $z = F(x, y)$.

\subsubsection*{Tangent Plane Of Surface}

$z = z_0 + F_x (x_0, y_0) (x - x_0) + F_y (x_0, y_0)(y - y_0)$

\subsubsection*{Normal Vector To Surface}

$
\begin{pmatrix}
  F_x (x_0, y_0) \\
  F_y (x_0, y_0) \\
  -1
\end{pmatrix}
$


\subsection*{Total Differential Approximation}

$\triangle F \approx F_x (x_0, y_0) (x - x_0) + F_y (x_0, y_0) (y - y_0)$


\subsection*{Chain Rule}

For a function $F$ with two variables $x$ and $y$, the chain rule can be
defined as
$$
\dfrac{dF}{dt} = \dfrac{\partial F}{\partial x} \dfrac{dx}{dt} +
\dfrac{\partial F}{\partial y} \dfrac{dy}{dt}
$$

Don't forget to substitute in $x$ and $y$ after finding the derivatives.


\subsection*{Functions Of Two Or More Variables}

\subsubsection*{Partial Derivatives}

For a function $F$ of three variables $x$, $y$ and $z$, the partial derivatives
of $F$ can be defined as

$$
F_x = \dfrac{\partial F}{\partial x}, \quad F_y = \dfrac{\partial F}{\partial
y}, \quad F_z = \dfrac{\partial F}{\partial z}
$$

\subsubsection*{Chain Rule}

For a function $F$ of three variables $x$, $y$ and $z$, where $x$ and $y$ are
each functions of both $u$ and $v$, the chain rule for $F$ can be defined as

$$
\dfrac{\partial F}{\partial u} = \dfrac{\partial F}{\partial x}
\dfrac{\partial x}{\partial u} + \dfrac{\partial F}{\partial y}
\dfrac{\partial y}{\partial u} + \dfrac{\partial F}{\partial z}
\dfrac{\partial z}{\partial u}
$$

$$
\dfrac{\partial F}{\partial v} = \dfrac{\partial F}{\partial x}
\dfrac{\partial x}{\partial v} + \dfrac{\partial F}{\partial y}
\dfrac{\partial y}{\partial v} + \dfrac{\partial F}{\partial z}
\dfrac{\partial x}{\partial v}
$$


\subsection*{Integration Techniques}

\subsubsection*{Trigonometric Integrals}

Considers integrals of the form $$\int \cos^m x \ \sin^n x \ dx$$

\textbf{Cases:}

\begin{enumerate}
  \tightlist{}
  \item $m$ or $n$ or both are odd: $u = \sin x$, $du = \cos x \ dx$
  \item $m$ and $n$ are even: $\cos^2x = \dfrac{1 + \cos 2x}{2}$,
  $\sin^2x = \dfrac{1 - \cos 2x}{2}$
\end{enumerate}


\subsection*{Ordinary Differential Equations}

\subsubsection*{Seperable ODEs}

Seperable ODEs are differential equations where two variables are involved
(usually $x$ and $y$) that can be seperated so that all the $y$'s are on one
side, and all the $x$'s are on the other. The tend to be in the form
$$f(y) \frac{dy}{dx} = g(x)$$

\textbf{To solve:}

\begin{enumerate}
  \item Move all the $y$'s to one side, and the $x$'s to the other

  So $f(y) \dfrac{dx}{dy} = g(x)$ becomes $f(y) dy = g(x) dx$

  \item Integrate both sides with respect to the respective variable

  $\int f(y) dy = \int g(x) dx$

  \item Solve for $y$
\end{enumerate}

\subsubsection*{First Order Linear ODEs}

First Order Linear ODEs are differential equations that involve functions of a
single variable. They can be written in the form $$\dfrac{dy}{dx} + f(x) y =
g(x)$$

\textbf{To solve:}

\begin{enumerate}
  \tightlist{}
  \item Write the differential equation as above
  \item Find the integrating factor $e^{\int f(x) dx}$, this is denoted by
  $h(x)$
  \item Multiply both sides by the integrating factor $h(x)$ to obtain
  $\dfrac{d}{dx} (h(x) y) = g(x) h(x)$
  \item Integrate both sides with respect to $x$, and solve for $y$
\end{enumerate}

\subsubsection*{Exact ODEs}

Exact ODEs are differentiable equations involving functions with two or more
variables. They are typically of the form $$F(x, y) + G(x, y) \frac{dy}{dx} =
0$$ and are said to be \textit{exact} if $$\dfrac{\partial F}{\partial y} =
\dfrac{\partial G}{\partial x}$$

\textbf{To solve:}

\begin{enumerate}
  \tightlist{}
  \item Show that a differential equation is exact by proving the above
  property
  \item Look for a function $H(x, y)$ such that
  $$
  \def\arraystretch{2}
  \begin{array}{l}
    \dfrac{\partial H}{\partial x} = F(x, y) \qquad (1) \\
    \dfrac{\partial H}{\partial y} = G(x, y) \qquad (2)
  \end{array}
  $$

  \item Integrate (1) with respect to $x$ to find $H(x, y) = f(x, y) + C(y)$
  \item To find $C(y)$, partially differentiate $H(x, y)$ with respect to $y$
  (leaving all of the $x$ components constant) and compare that with the
  partial derivative of $H$ with respect to $y$. This gives $C'(y)$ and thus
  allows to find $C(y)$
\end{enumerate}


\subsection*{Second Order Linear ODEs}

A second order linear ODE with constant coefficients is said to be homogeneous
if it is of the form $$ y'' + ay' + by = 0 $$ where $a$ and $b$ are real
numbers.

\subsubsection*{Characteristic Equation}

The characteristic equation of a second order linear ODE is given by
$$\lambda^2 + a\lambda + b = 0$$


\subsection*{Taylor Polynomial}

For a differentiable function $f$, the Taylor polynomial $p_n$ of
order $n$ at $x = a$ is
$$
p_n (x) = f(a) = f'(a) (x - a) + \frac{f''(a)}{2!} (x - a)^2 + \frac{f^{(3)}
(a)}{3!} (x - a)^3 + \dots{} + \frac{f^{(n)} (a)}{n!} (x - a)^n
$$

\subsubsection*{Taylor's Theorem}

$$f(x) = p_n(x) + R_{n + 1} (x)$$

where

$$R_{n + 1} (x) = \frac{f^{(n + 1)} (c)}{(n + 1)!} (x - a)^{n + 1}$$


\subsection*{Sequences}

When evaluating limits, functions and sequences are identical. This is shown
below $$\lim_{x \to \infty} f(x) = L \implies \lim_{n \to \infty} a_n = L$$

A sequence diverges when $\lim_{n \to \infty} a_n \pm \infty$ or
$\lim_{n \to \infty} a_n$ does not exist. Otherwise, the sequence converges.

\subsubsection*{Properties Of Sequences}

Given a sequence of real numbers $\{ a_n \}_{n = 0}^{\infty}$, the following
properties hold

\begin{itemize}
  \tightlist{}
  \item \textit{increasing} if $a_n < a_{n + 1}$ for each $n \in \mathbb{N}$
  \item \textit{non-decreasing} if $a_n \le a_{n + 1}$ for each
  $n \in \mathbb{N}$
  \item \textit{decreasing} if $a_n > a_{n + 1}$ for each $n \in \mathbb{N}$
  \item \textit{non-increasing} if $a_n \ge a_{n + 1}$ for each
  $n \in \mathbb{N}$
  \item $M$ is a \textit{upper bound} if $a_n \le M$ for each
  $n \in \mathbb{N}$
  \item $M$ is a \textit{lower bound} if $a_n \ge M$ for each
  $n \in \mathbb{N}$
\end{itemize}


\subsection*{Infinite Series}

\subsubsection*{The $k$th Term Divergence Test}

$\sum_{k = 1}^{\infty} a_k$ diverges if $\lim_{n \to \infty} a_k$ fails to
exist, or is non-zero.

\subsubsection*{Integral Test}

\subsubsection*{Comparison Test}

Suppose that $\{ a_k \}_{k = 0}^{\infty}$ and $\{ b_k \}_{k = 0}^{\infty}$ are
two positive sequences such that $ak \le bk$ for every natural number $k$.

\begin{itemize}
  \tightlist{}
  \item If $\sum_{k = 0}^{\infty} b_k$ converges, then $\sum_{k = 0}^{\infty}
  a_k$ converges
  \item If $\sum_{k = 0}^{\infty} b_k$ diverges, then $\sum_{k = 0}^{\infty}
  a_k$ diverges
\end{itemize}

Usually used for series of the form $\sum_{k = 1}^{\infty} \dfrac{1}{k^p}$,
such that this series converges if $p > 1$ and diverges if $p \le 1$.

\subsubsection*{Ratio Test}

Suppose that $\sum a_k$ is an infinite series with positive terms and that
$\lim_{k \to \infty} \dfrac{a_{k + 1}}{a_k} = r$.

\begin{itemize}
  \tightlist{}
  \item If $r < 1$ then $\sum a_k$ converges
  \item If $r > 1$ then $\sum a_k$ diverges
  \item If $r = 1$ this test is inconclusive
\end{itemize}

\subsubsection*{Alternating Series Test}

\subsection*{Taylor Series}

$\displaystyle \sum_{k = 0}^{\infty} \frac{f^{(k)} (a)}{k!} (x - a)^k$

\subsection*{Power Series}

Given a sequence $\{ a_k \}_{k = 0}^{\infty}$ is a sequence of real numbers and
that $a \in \mathbb{R}$, then

$$\sum_{k = 0}^{\infty} a_k x^k$$

is the power series of $x$, and

$$\sum_{k = 0}^{\infty} a_k (x - a)^k$$

is a power series of $x - a$.

\end{document}
```